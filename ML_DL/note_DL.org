* 非极大值抑制(non maximum suppression)
抑制过程:迭代-遍历-消除\\

(1)将所有框的得分排序,选中最高分的框\\

(2)便利其余的框,如果和当前最高分框的重叠面积(IOU)大于一定阈值,将其删除\\

(3)从未处理的框中继续选一个得分最高的,重复上述过程.\\

* 支持向量机(Support Vector Machine)
支持向量机有三宝:jiange对偶和技巧

** hard-margin SVM
** soft-margin SVM
** kernel SVM
* 微调(fine-tuning)
在实践中，由于数据集不够大，很少有人从头开始训练网络。常见的做法是使用预训练的网络（例如在ImageNet上训练的分类1000类的网络）来重新fine-tuning（也叫微调），或者当做特征提取器。

以下是常见的两类迁移学习场景：

1卷积网络当做特征提取器。使用在ImageNet上预训练的网络，去掉最后的全连接层，剩余部分当做特征提取器（例如AlexNet在最后分类器前，是4096维的特征向量）。这样提取的特征叫做CNN codes。得到这样的特征后，可以使用线性分类器（Liner SVM、Softmax等）来分类图像。

2 Fine-tuning卷积网络。替换掉网络的输入层（数据），使用新的数据继续训练。Fine-tune时可以选择fine-tune全部层或部分层。通常，前面的层提取的是图像的通用特征（generic features）（例如边缘检测，色彩检测），这些特征对许多任务都有用。后面的层提取的是与特定类别有关的特征，因此fine-tune时常常只需要Fine-tuning后面的层。

预训练模型 

在ImageNet上训练一个网络，即使使用多GPU也要花费很长时间。因此人们通常共享他们预训练好的网络，这样有利于其他人再去使用。例如，Caffe有预训练好的网络地址Model Zoo。

何时以及如何Fine-tune

决定如何使用迁移学习的因素有很多，这是最重要的只有两个：新数据集的大小、以及新数据和原数据集的相似程度。有一点一定记住：网络前几层学到的是通用特征，后面几层学到的是与类别相关的特征。这里有使用的四个场景：

1、新数据集比较小且和原数据集相似。因为新数据集比较小，如果fine-tune可能会过拟合；又因为新旧数据集类似，我们期望他们高层特征类似，可以使用预训练网络当做特征提取器，用提取的特征训练线性分类器。

2、新数据集大且和原数据集相似。因为新数据集足够大，可以fine-tune整个网络。

3、新数据集小且和原数据集不相似。新数据集小，最好不要fine-tune，和原数据集不类似，最好也不使用高层特征。这时可是使用前面层的特征来训练SVM分类器。

4、新数据集大且和原数据集不相似。因为新数据集足够大，可以重新训练。但是实践中fine-tune预训练模型还是有益的。新数据集足够大，可以fine-tine整个网络。

实践建议

预训练模型的限制。使用预训练模型，受限于其网络架构。例如，你不能随意从预训练模型取出卷积层。但是因为参数共享，可以输入任意大小图像；卷积层和池化层对输入数据大小没有要求（只要步长stride fit），其输出大小和属于大小相关；全连接层对输入大小没有要求，输出大小固定。

学习率。与重新训练相比，fine-tune要使用更小的学习率。因为训练好的网络模型权重已经平滑，我们不希望太快扭曲（distort）它们（尤其是当随机初始化线性分类器来分类预训练模型提取的特征时）。

* Lenex
模型如图:\\
[[./figure/LeNet.png]]\\
一共7层3个卷积层,2个下采样层,一个全连接层,和一个输出层.\\
C3层用的不是一个通道数为6的卷积核,而是6x(3x5x5+1)+9x(4x5x5+1)+1x(6x5x5+1)\\
|------|
| 3456 |        
| 7890 |
| fjei |
| djiw |
|------|
h_{1,a}=f(x*w+b)\\
h_{2,a}=down(h_{1,a})\\
* AlexNet
[[./figure/AlexNet.png]]\\
1.ReLU\\
2.GPU\\
3.重叠池化\\
4.dropout\\
5.局部响应归一化\\
* VGGNet
[[./figure/VGG2.jpg]]\\
[[./figure/VGG1.jpg]]\\
* GoogleLeNet
BN(Batch Normalization):1.求出平均数2.求出方差3.(x-avg)/(方差+微笑数)\\
Inception模块:增加网络的宽度同时减少参数.\\
[[./figure/inception1]]\\

[[./figure/inception2]]\\
增加了网络的宽度,是增加了网络对尺度的适应性,不同的支路感受野是不同的有多种尺度的信息在里面\\
[[./figure/google]]\\
* SPPNet
[[./figure/SPP1.png]]\\
全连接层导致输入图像必须相同.\\
单尺度训练多尺度训练\\
[[./figure/SPP2.png]]\\
* highwayNet
解决过深的网络产生的梯度消失.\\
用门电路表示是否舍弃当前层输入\\
* ResNet
[[./figure/ResNet]]\\
如果维数不同需要引入一个权值矩阵进行映射或者点卷积.
* DenseNet
[[./figure/DenseNet.jpeg]]\\
如果每层都有k个特征图那么,第l层就有k(l-1)+k0个特征图,通常k会被限制是一个小数,每个3x3卷积核卷积操作之前引入一个1x1卷积核进行降维.
* CatNet
不同尺度不同层进行拼接,交错的卷积层和池化层.
* R-CNN
1.Region proposal(selective search)\\
2.特征提取\\
3.区域分类输入一张图根据像素相似度提取两千个框,压缩crop,做2000次卷积,生成2000个特征图,对每个特征图使用1000个SVM做二分类打分,根据SVM提供的分数,进行nms,然后regression求框的位置.\\
[[./figure/r-cnn.jpg]]

* Fast R-CNN
1.加入SPPNet\\
2.映射2000个框,只做一次特征提取\\
3.两个分类:交叉熵求类别,欧式距离求边框回归器.
* Faster R-CNN
[[./figure/faster-rcnn.jpg]]\\
1.RPN\\
RPN层分出两个网络,一张特征图(不算通道)每个像素为一个滑块,设一个滑块k个anchors,共用(特征图xanchors)个候选框(128^2,256^2 512^2)对这些候选框进行分类打分(2k)以及回归(x,y,h,w)\\

* YOLO
检测系统将输入图像分成SxS个网格,每个网格负责预测中心落在其中的对象目标并预测B个边框及相应的置信得分.SxS[2x5+20]每个网格两个bbox,置信度为交并比不包含对象为0.\\
损失函数分为4部分:中心坐标,宽高,自信度,分类
* SSD
在yolo的基础上添加了多尺度,yolo的分割方式,faster-rcnn的取archors方式,CatNet的拼接方式,
* FCN
没有全连接层,产生热力图.deconverlution(transfer deconverlution),空洞卷积\\
拼接放大拼接不同的层产生一个密集输出
* PSPNet
并行生成不同的特征图,拼接,小特征图双线性插值上采样.
* Mask R-CNN
语义分割:对每个像素分类\\
实例分割:每个像素分类,不同实例区分开来\\
1.在Faster-rcnn基础上增加了ROIAlign层(在rpn推荐的ROI中不是用传统的一次SPP,而是用更加细粒度的双线性差值进行池化)\\
2.利用Mast对输入对象的空间布局进行编码(每个ROI生成一个mxm的mask,mask有n个通道(类别数))每个ROI产生一个mxmxn个输出,平均二值交叉熵计算损失\\
* SiameseNet
判断两个输入是否相似
* SqueezeNet
类似inception模块
* DCGAN
一个生成模型,一个判别模型
* NIN
微网络
